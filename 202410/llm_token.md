# LLM(대규모 언어 모델)에서의 토큰(Token) 이해하기

## 1. 토큰이란 무엇인가?

LLM(Large Language Model)은 텍스트 데이터를 처리하고 이해할 때 '토큰(token)'이라는 기본 단위를 사용함. 토큰은 단어, 부분 단어, 심지어 문장 부호 등으로 구성되며, 모델이 입력 텍스트를 학습하거나 예측할 때 사용하는 최소 단위임.

예를 들어, 다음과 같은 문장이 있다고 가정함:
> "오늘 날씨가 정말 좋다."

이 문장을 토큰으로 나누면 다음과 같을 수 있음:
- "오늘", "날씨", "가", "정말", "좋", "다", "."

토큰화 과정에서 단어를 부분적으로 나누는 이유는 모델이 더 작은 단위로 텍스트를 처리함으로써 더 유연하게 다양한 언어 패턴을 학습할 수 있기 때문임.

## 2. 토큰화(Tokenization)

토큰화를 통해 문장을 토큰으로 분리하는 과정이 이루어짐. 이때 사용하는 알고리즘은 모델에 따라 다를 수 있음. 대표적인 토큰화 방법은 다음과 같음:

- **단어 수준 토큰화**: 각 단어를 개별 토큰으로 처리함.
- **부분 단어 토큰화**: 단어를 더 작은 의미 단위로 분할하여 처리함. BPE(Byte Pair Encoding)와 같은 알고리즘이 대표적임.
- **문장 부호 및 특수 문자 처리**: 문장 부호, 숫자, 특수 기호 등을 별도의 토큰으로 처리함.

### BPE(Byte Pair Encoding)
BPE는 자주 등장하는 글자 쌍을 하나의 토큰으로 합치는 방식으로, 희소한 단어를 다루는 데 유용함. 예를 들어, "좋다"라는 단어는 처음에는 '좋', '다'로 나뉘지만, 학습이 진행되면서 자주 등장하는 패턴을 하나의 토큰으로 결합함.

## 3. LLM에서의 토큰 사용

대규모 언어 모델은 입력 텍스트를 토큰으로 분리한 후 이를 숫자로 변환하여 학습에 사용함. 각 토큰은 고유한 ID를 가지며, 모델은 이 ID를 통해 단어 간의 관계를 학습함. 모델의 예측도 이 토큰 단위로 이루어지며, 결과적으로 새로운 텍스트를 생성하거나 질문에 답할 때도 토큰 단위로 처리함.

### 토큰의 중요성
- **모델의 효율성**: 토큰의 크기와 수는 모델의 학습 및 추론 속도에 직접적인 영향을 미침. 짧은 토큰을 많이 사용하는 경우 처리 시간이 증가할 수 있음.
- **맥락 이해**: 토큰 단위가 지나치게 작으면 모델이 문맥을 파악하는 데 어려움을 겪을 수 있음. 반대로 너무 크면 희귀한 단어를 효과적으로 처리하지 못할 수 있음.

<a href="https://www.flaticon.com/kr/free-icons/ai-" title="ai 애플리케이션 아이콘">Ai 애플리케이션 아이콘 제작자: Iconfromus - Flaticon</a>